{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing and Cleaning Data - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous labs, you joined the data from our separate files into a single DataFrame.  In this lab, you'll scrub the data to get it ready for exploration and modeling!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Cast columns to the appropriate data types\n",
    "* Identify and deal with null values appropriately\n",
    "* Remove unnecessary columns\n",
    "* Understand how to normalize data\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "You'll find the resulting dataset from our work in the _Obtaining Data_ Lab stored within the file `walmart_data_not_cleaned.csv`.  \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import pandas and set the standard alias\n",
    "* Import numpy and set the standard alias\n",
    "* Import matplotlib.pyplot and set the standard alias\n",
    "* Import seaborn and set the alias `sns` (this is the standard alias for seaborn)\n",
    "* Use the ipython magic command to set all matplotlib visualizations to display inline in the notebook\n",
    "* Load the dataset stored in the .csv file into a DataFrame using pandas\n",
    "* Inspect the head of the DataFrame to ensure everything loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements go here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_id</th>\n",
       "      <th>ages</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>set_name</th>\n",
       "      <th>prod_desc</th>\n",
       "      <th>prod_long_desc</th>\n",
       "      <th>theme_name</th>\n",
       "      <th>country</th>\n",
       "      <th>list_price</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>play_star_rating</th>\n",
       "      <th>review_difficulty</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>val_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75823</td>\n",
       "      <td>6-12</td>\n",
       "      <td>277</td>\n",
       "      <td>Bird Island Egg Heist</td>\n",
       "      <td>Catapult into action and take back the eggs fr...</td>\n",
       "      <td>Use the staircase catapult to launch Red into ...</td>\n",
       "      <td>Angry Birds™</td>\n",
       "      <td>US</td>\n",
       "      <td>$29.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75822</td>\n",
       "      <td>6-12</td>\n",
       "      <td>168</td>\n",
       "      <td>Piggy Plane Attack</td>\n",
       "      <td>Launch a flying attack and rescue the eggs fro...</td>\n",
       "      <td>Pilot Pig has taken off from Bird Island with ...</td>\n",
       "      <td>Angry Birds™</td>\n",
       "      <td>US</td>\n",
       "      <td>$19.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Easy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75821</td>\n",
       "      <td>6-12</td>\n",
       "      <td>74</td>\n",
       "      <td>Piggy Car Escape</td>\n",
       "      <td>Chase the piggy with lightning-fast Chuck and ...</td>\n",
       "      <td>Pitch speedy bird Chuck against the Piggy Car....</td>\n",
       "      <td>Angry Birds™</td>\n",
       "      <td>US</td>\n",
       "      <td>$12.99</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Easy</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21030</td>\n",
       "      <td>12+</td>\n",
       "      <td>1032</td>\n",
       "      <td>United States Capitol Building</td>\n",
       "      <td>Explore the architecture of the United States ...</td>\n",
       "      <td>Discover the architectural secrets of the icon...</td>\n",
       "      <td>Architecture</td>\n",
       "      <td>US</td>\n",
       "      <td>$99.99</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Average</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21035</td>\n",
       "      <td>12+</td>\n",
       "      <td>744</td>\n",
       "      <td>Solomon R. Guggenheim Museum®</td>\n",
       "      <td>Recreate the Solomon R. Guggenheim Museum® wit...</td>\n",
       "      <td>Discover the architectural secrets of Frank Ll...</td>\n",
       "      <td>Architecture</td>\n",
       "      <td>US</td>\n",
       "      <td>$79.99</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Challenging</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prod_id  ages  piece_count                        set_name  \\\n",
       "0    75823  6-12          277           Bird Island Egg Heist   \n",
       "1    75822  6-12          168              Piggy Plane Attack   \n",
       "2    75821  6-12           74                Piggy Car Escape   \n",
       "3    21030   12+         1032  United States Capitol Building   \n",
       "4    21035   12+          744   Solomon R. Guggenheim Museum®   \n",
       "\n",
       "                                           prod_desc  \\\n",
       "0  Catapult into action and take back the eggs fr...   \n",
       "1  Launch a flying attack and rescue the eggs fro...   \n",
       "2  Chase the piggy with lightning-fast Chuck and ...   \n",
       "3  Explore the architecture of the United States ...   \n",
       "4  Recreate the Solomon R. Guggenheim Museum® wit...   \n",
       "\n",
       "                                      prod_long_desc    theme_name country  \\\n",
       "0  Use the staircase catapult to launch Red into ...  Angry Birds™      US   \n",
       "1  Pilot Pig has taken off from Bird Island with ...  Angry Birds™      US   \n",
       "2  Pitch speedy bird Chuck against the Piggy Car....  Angry Birds™      US   \n",
       "3  Discover the architectural secrets of the icon...  Architecture      US   \n",
       "4  Discover the architectural secrets of Frank Ll...  Architecture      US   \n",
       "\n",
       "  list_price  num_reviews  play_star_rating review_difficulty  star_rating  \\\n",
       "0     $29.99          2.0               4.0           Average          4.5   \n",
       "1     $19.99          2.0               4.0              Easy          5.0   \n",
       "2     $12.99         11.0               4.3              Easy          4.3   \n",
       "3     $99.99         23.0               3.6           Average          4.6   \n",
       "4     $79.99         14.0               3.2       Challenging          4.6   \n",
       "\n",
       "   val_star_rating  \n",
       "0              4.0  \n",
       "1              4.0  \n",
       "2              4.1  \n",
       "3              4.3  \n",
       "4              4.1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, load in the dataset and inspect the head to make sure everything loaded correctly\n",
    "df = pd.read_csv('Lego_data_merged.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting our Data Cleaning\n",
    "\n",
    "To start, you'll deal with the most obvious issue: data features with the wrong data encoding.\n",
    "\n",
    "### Checking Data Types\n",
    "\n",
    "In the cell below, use the appropriate method to check the data type of each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10870 entries, 0 to 10869\n",
      "Data columns (total 14 columns):\n",
      "prod_id              10870 non-null int64\n",
      "ages                 10870 non-null object\n",
      "piece_count          10870 non-null int64\n",
      "set_name             10870 non-null object\n",
      "prod_desc            10512 non-null object\n",
      "prod_long_desc       10870 non-null object\n",
      "theme_name           10870 non-null object\n",
      "country              10870 non-null object\n",
      "list_price           10870 non-null object\n",
      "num_reviews          9449 non-null float64\n",
      "play_star_rating     9321 non-null float64\n",
      "review_difficulty    9104 non-null object\n",
      "star_rating          9449 non-null float64\n",
      "val_star_rating      9301 non-null float64\n",
      "dtypes: float64(4), int64(2), object(8)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, investigate some of the unique values inside of the `list_price` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$24.3878    565\n",
       "$36.5878    520\n",
       "$12.1878    515\n",
       "$18.2878    304\n",
       "$42.6878    234\n",
       "Name: list_price, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.list_price.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data Stored as Strings\n",
    "\n",
    "A common issue to check for at this stage is numeric columns that have accidentally been encoded as strings. For example, you should notice that the `list_price` column above is currently formatted as a string and contains a proceeding '$'. Remove this and convert the remaining number to a `float` so that you can later model this value. After all, your primary task is to generate model to predict the price.\n",
    "\n",
    "> Note: While the data spans a multitude of countries, assume for now that all prices have been standardized to USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.99, 19.99, 12.99, 99.99, 79.99])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; extract the list_price as a floating number\n",
    "df.list_price = df.list_price.map(lambda x : float(x[1:]))\n",
    "df.list_price.unique()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Dealing With Null Values\n",
    "\n",
    "Next, it's time to check for null values. How to deal with the null values will be determined by the columns containing them, and how many null values exist in each.  \n",
    " \n",
    "In the cell below, get a count of how many null values exist in each column in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10870 entries, 0 to 10869\n",
      "Data columns (total 14 columns):\n",
      "prod_id              10870 non-null int64\n",
      "ages                 10870 non-null object\n",
      "piece_count          10870 non-null int64\n",
      "set_name             10870 non-null object\n",
      "prod_desc            10512 non-null object\n",
      "prod_long_desc       10870 non-null object\n",
      "theme_name           10870 non-null object\n",
      "country              10870 non-null object\n",
      "list_price           10870 non-null float64\n",
      "num_reviews          9449 non-null float64\n",
      "play_star_rating     9321 non-null float64\n",
      "review_difficulty    9104 non-null object\n",
      "star_rating          9449 non-null float64\n",
      "val_star_rating      9301 non-null float64\n",
      "dtypes: float64(5), int64(2), object(7)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info() #Same as checking the type above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get some descriptive statistics for each of the columns. You want to see where the minimum and maximum values lie.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prod_id</th>\n",
       "      <th>piece_count</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>play_star_rating</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>val_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.087000e+04</td>\n",
       "      <td>10870.000000</td>\n",
       "      <td>9449.000000</td>\n",
       "      <td>9321.000000</td>\n",
       "      <td>9449.000000</td>\n",
       "      <td>9301.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.181634e+04</td>\n",
       "      <td>503.936431</td>\n",
       "      <td>17.813737</td>\n",
       "      <td>4.355413</td>\n",
       "      <td>4.510319</td>\n",
       "      <td>4.214439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.736390e+05</td>\n",
       "      <td>831.209318</td>\n",
       "      <td>38.166693</td>\n",
       "      <td>0.617272</td>\n",
       "      <td>0.516463</td>\n",
       "      <td>0.670906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.300000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.112300e+04</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.207350e+04</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.124800e+04</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000431e+06</td>\n",
       "      <td>7541.000000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            prod_id   piece_count  num_reviews  play_star_rating  star_rating  \\\n",
       "count  1.087000e+04  10870.000000  9449.000000       9321.000000  9449.000000   \n",
       "mean   6.181634e+04    503.936431    17.813737          4.355413     4.510319   \n",
       "std    1.736390e+05    831.209318    38.166693          0.617272     0.516463   \n",
       "min    6.300000e+02      1.000000     1.000000          1.000000     1.800000   \n",
       "25%    2.112300e+04     97.000000     2.000000          4.000000     4.300000   \n",
       "50%    4.207350e+04    223.000000     6.000000          4.500000     4.600000   \n",
       "75%    7.124800e+04    556.000000    14.000000          4.800000     5.000000   \n",
       "max    2.000431e+06   7541.000000   367.000000          5.000000     5.000000   \n",
       "\n",
       "       val_star_rating  \n",
       "count      9301.000000  \n",
       "mean          4.214439  \n",
       "std           0.670906  \n",
       "min           1.000000  \n",
       "25%           4.000000  \n",
       "50%           4.300000  \n",
       "75%           4.700000  \n",
       "max           5.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a bit more of a understanding of each of these features you can now make an informed decision about the best strategy for dealing with the various null values. \n",
    "\n",
    "* The data contained within each column are continuously-valued floats. \n",
    "* The range is quite large, with the smallest value being around 0 or even negative in some columns, and the max being greater than 100,000.\n",
    "* There is extremely high variance in each, with the standard deviation being larger than the mean in all 5 columns. \n",
    "\n",
    "\n",
    "### Dealing With Null Values Through Binning\n",
    "\n",
    "This suggests that the best bet is to bin the columns.\n",
    "For now, start with with 5 bins of equal size. \n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Create a binned version of each `MarkDown` column and add them to the DataFrame.  \n",
    "* When calling `pd.cut()`, pass in the appropriate column as the object to be binned, the number of bins we want, `5`, and set the `labels` parameter to `bins`, so that you have clearly labeled names for each bin. \n",
    "\n",
    "For more information on how to bin these columns using pd.cut, see the [pandas documentation for this method.](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = ['0-20%', '21-40%', '41-60%', '61-80%', '81-100%']\n",
    "\n",
    "for i in range (1, 6):\n",
    "    df[\"binned_markdown_\" + str(i)] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, check the `.dtypes` attribute of the DataFrame to see that these new categorical columns have been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They exist! However, they still contain null values.  You need to replace all null values with a string that will represent all missing values.  Use the `replace()` method or the `fillna()` method on each column and replace `NaN` with `\"NaN\"`. \n",
    "\n",
    "In the cell below, replace all missing values inside our `binned_markdown` columns with the string `\"NaN\"`.\n",
    "\n",
    "**_NOTE:_** If you're unsure of how to do this, check the [pandas documentation for replace](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,6):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, check if those columns still contain null values. \n",
    "\n",
    "In the cell below, display the number of null values contained within each column of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! You've now dealt with all the null values in the dataset through **_Coarse Classification_** by binning the data and treating null values as a distinct category. All that's left to do is to drop our original `MarkDown` columns from the DataFrame. \n",
    "\n",
    "Note that in this step, you'll also drop the `Date` column, because you are going to build a generalized model and will not be making use of any time series data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a list called `to_drop` that contains the name of every `MarkDown` column you need to drop (for a challenge, try doing this with a list comprehension!)\n",
    "* Append `\"Date\"` to `to_drop`\n",
    "* Drop these columns (in place) from the DataFrame\n",
    "* Display the number of null values in each column again to confirm that these columns have been dropped, and that the DataFrame now contains no missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Multicollinearity\n",
    "\n",
    "\n",
    "Before you one-hot encode the categorical columns usin `pd.get_dummies()`, you'll want to quickly check the dataset for multicollinearity, since this can severly impact model stability and interpretability.  You want to make sure that the columns within the dataset are not highly correlated. \n",
    "\n",
    "A good way to check for multicollinearity between features is to create a correlation heatmap.\n",
    "\n",
    "The [seaborn documentation](https://seaborn.pydata.org/examples/many_pairwise_correlations.html) provides some great code samples to help you figure out how to display a Correlation Heatmap.  \n",
    "\n",
    "Check out this documentation, and then modify the code included below so that it displays a Correlation Heatmap for your dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of the visualization\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create a covariance matrix\n",
    "corr = None\n",
    "\n",
    "# Generate a mask the size of our covariance matrix\n",
    "mask = None\n",
    "mask[np.triu_indices_from(mask)] = None\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = None\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = None\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the Correlation Heatmap you created above to answer the following questions:\n",
    "\n",
    "Which columns are highly correlated with the target column our model will predict?  Are any of our predictor columns highly correlated enough that we should consider dropping them?  Explain your answer.\n",
    "\n",
    "Write your answer below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing the Data\n",
    "\n",
    "Now, you'll need to convert all of our numeric columns to the same scale by **_normalizing_** our dataset.  Recall that you normalize a dataset by converting each numeric value to it's corresponding z-score for the column, which is obtained by subtracting the column's mean and then dividing by the column's standard deviation for every value. \n",
    "\n",
    "Since you only have 4 columns containing numeric data that needs to be normalized, you can do this by hand in the cell below. This avoids errors that stem from trying to normalize datasets that contain strings in all of our categorical columns. Plus, it's good practice to help remember how normalization works!\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Normalize the following columns individually: `Size`, `Temperature`, `Fuel_Price`, `CPI`, and `Unemployment` by subtracting the column mean and dividing by the column standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Size = None\n",
    "df.Temperature = None\n",
    "df.Fuel_Price = None\n",
    "df.CPI = None\n",
    "df.Unemployment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding Categorical Columns\n",
    "\n",
    "As a final step, you'll need to deal with the categorical columns by **_one-hot encoding_** them into binary variables via the `pd.get_dummies()` method.  \n",
    "\n",
    "In the cell below, use the [`pd.get_dummies()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) to one-hot encode the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've now successfully scrubbed your dataset--you're now ready for data exploration and modeling!\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you learned gain practice with data cleaning by:\n",
    "\n",
    "* Casting columns to the appropriate data types\n",
    "* Identifying and deal with null values appropriately\n",
    "* Removing unnecessary columns\n",
    "* Checking for and deal with multicollinearity\n",
    "* Normalizing your data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
